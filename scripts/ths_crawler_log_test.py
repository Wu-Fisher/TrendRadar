#!/usr/bin/env python3
"""
åŒèŠ±é¡º7x24å°æ—¶è¦é—»ç›´æ’­çˆ¬è™« - è¯¦ç»†æ—¥å¿—æµ‹è¯•
è¿è¡Œ 10 åˆ†é’Ÿï¼Œ30 ç§’é—´éš”ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—åˆ° txt æ–‡ä»¶
"""

import requests
import json
import re
import time
from datetime import datetime
from typing import Optional
import pytz


class THSNewsCrawler:
    """åŒèŠ±é¡ºå®æ—¶æ–°é—»çˆ¬è™«"""

    DATA_URL = "http://stock.10jqka.com.cn/thsgd/realtimenews.js"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://news.10jqka.com.cn/realtimenews.html",
        })
        self.seen_seqs: set[int] = set()
        self.latest_seq: Optional[int] = None
        self.tz = pytz.timezone('Asia/Shanghai')

    def get_cn_time(self) -> str:
        """è·å–ä¸­å›½æ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.now(self.tz).strftime('%Y-%m-%d %H:%M:%S')

    def fetch_raw_data(self) -> tuple[Optional[str], str]:
        """è·å–åŸå§‹æ•°æ®ï¼Œè¿”å› (æ•°æ®, çŠ¶æ€ä¿¡æ¯)"""
        try:
            resp = self.session.get(self.DATA_URL, timeout=10)
            resp.encoding = 'gbk'
            return resp.text, f"HTTP {resp.status_code}, {len(resp.text)} bytes"
        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError as e:
            return None, f"è¿æ¥é”™è¯¯: {e}"
        except Exception as e:
            return None, f"æœªçŸ¥é”™è¯¯: {e}"

    def parse_jsonp(self, raw_data: str) -> tuple[Optional[dict], str]:
        """è§£æ JSONP æ•°æ®ï¼Œè¿”å› (æ•°æ®, çŠ¶æ€ä¿¡æ¯)"""
        try:
            json_str = raw_data.strip()

            start = json_str.find('{')
            if start == -1:
                return None, "æ— æ³•æ‰¾åˆ° JSON èµ·å§‹ä½ç½®"

            end = json_str.rfind('};')
            if end != -1:
                json_str = json_str[start:end+1]
            else:
                end = json_str.rfind('}')
                if end != -1:
                    json_str = json_str[start:end+1]
                else:
                    json_str = json_str[start:]

            # ç»™å¤–å±‚å±æ€§ååŠ åŒå¼•å·
            json_str = re.sub(r'(\{)\s*(pubDate):', r'\1"\2":', json_str)
            json_str = re.sub(r',\s*(latestNewsSeq):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(counter):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(item):', r',"\1":', json_str)

            data = json.loads(json_str)
            item_count = len(data.get("item", []))
            return data, f"è§£ææˆåŠŸ, {item_count} æ¡æ–°é—», æœ€æ–°åºå·: {data.get('latestNewsSeq')}"
        except json.JSONDecodeError as e:
            return None, f"JSON è§£æå¤±è´¥: {e}"

    def extract_news_item(self, item: dict) -> dict:
        """æå–å¹¶å¤„ç†å•æ¡æ–°é—»"""
        return {
            "seq": item.get("seq"),
            "title": item.get("title", "").strip(),
            "content": item.get("content", "").strip(),
            "url": item.get("url", ""),
            "pub_date": item.get("pubDate", ""),
            "source": item.get("source", ""),
            "stocks": item.get("stocks"),
            "stock_code": item.get("stockCode", ""),
            "category": item.get("category", ""),
            "implevel": item.get("implevel", ""),  # é‡è¦ç¨‹åº¦
        }

    def get_all_news(self) -> tuple[list[dict], str, str]:
        """è·å–æ‰€æœ‰æ–°é—»ï¼Œè¿”å› (æ–°é—»åˆ—è¡¨, è·å–çŠ¶æ€, è§£æçŠ¶æ€)"""
        raw_data, fetch_status = self.fetch_raw_data()
        if not raw_data:
            return [], fetch_status, "æœªæ‰§è¡Œ"

        data, parse_status = self.parse_jsonp(raw_data)
        if not data:
            return [], fetch_status, parse_status

        items = [self.extract_news_item(item) for item in data.get("item", [])]
        return items, fetch_status, parse_status

    def get_incremental_news(self) -> tuple[list[dict], list[dict], str, str]:
        """è·å–å¢é‡æ–°é—»ï¼Œè¿”å› (æ–°å¢æ–°é—», å…¨éƒ¨æ–°é—», è·å–çŠ¶æ€, è§£æçŠ¶æ€)"""
        all_items, fetch_status, parse_status = self.get_all_news()

        new_items = []
        for item in all_items:
            seq = item.get("seq")
            if seq and seq not in self.seen_seqs:
                self.seen_seqs.add(seq)
                new_items.append(item)

        return new_items, all_items, fetch_status, parse_status


def format_news_detail(item: dict) -> str:
    """æ ¼å¼åŒ–å•æ¡æ–°é—»çš„è¯¦ç»†ä¿¡æ¯"""
    lines = []
    lines.append(f"    åºå·: {item['seq']}")
    lines.append(f"    æ ‡é¢˜: {item['title']}")
    lines.append(f"    å‘å¸ƒæ—¶é—´: {item['pub_date']}")
    lines.append(f"    æ¥æº: {item['source']}")
    lines.append(f"    é“¾æ¥: {item['url']}")
    if item['stock_code']:
        lines.append(f"    ç›¸å…³è‚¡ç¥¨: {item['stock_code']}")
    if item['implevel']:
        lines.append(f"    é‡è¦ç¨‹åº¦: {item['implevel']}")
    # å†…å®¹æˆªå–å‰ 200 å­—ç¬¦
    content = item['content']
    if len(content) > 200:
        content = content[:200] + "..."
    lines.append(f"    å†…å®¹æ‘˜è¦: {content}")
    return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œ 10 åˆ†é’Ÿçš„è¯¦ç»†æ—¥å¿—æµ‹è¯•"""
    # é…ç½®
    duration = 10 * 60  # 10 åˆ†é’Ÿ
    interval = 30  # 30 ç§’é—´éš”

    # è¾“å‡ºæ–‡ä»¶
    output_file = "/home/wufisher/ws/dev/TrendRadar/output/ths_crawler_test_log.txt"

    crawler = THSNewsCrawler()

    with open(output_file, 'w', encoding='utf-8') as f:
        def log(msg: str):
            """åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
            print(msg, flush=True)
            f.write(msg + "\n")
            f.flush()

        log("=" * 80)
        log("åŒèŠ±é¡º 7x24 å°æ—¶è¦é—»ç›´æ’­ - è¯¦ç»†æ—¥å¿—æµ‹è¯•")
        log("=" * 80)
        log(f"å¼€å§‹æ—¶é—´: {crawler.get_cn_time()}")
        log(f"æµ‹è¯•æ—¶é•¿: {duration // 60} åˆ†é’Ÿ")
        log(f"æ£€æŸ¥é—´éš”: {interval} ç§’")
        log(f"æ•°æ®æº: {THSNewsCrawler.DATA_URL}")
        log(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        log("=" * 80)
        log("")

        # ========== é¦–æ¬¡è·å–ï¼ˆå»ºç«‹åŸºçº¿ï¼‰==========
        log(f"[{crawler.get_cn_time()}] === é¦–æ¬¡è·å–ï¼ˆå»ºç«‹åŸºçº¿ï¼‰===")
        all_items, fetch_status, parse_status = crawler.get_all_news()
        log(f"  è·å–çŠ¶æ€: {fetch_status}")
        log(f"  è§£æçŠ¶æ€: {parse_status}")

        if all_items:
            log(f"  è·å–åˆ° {len(all_items)} æ¡æ–°é—»")

            # è®°å½•æ‰€æœ‰å·²è§åºå·
            for item in all_items:
                if item.get("seq"):
                    crawler.seen_seqs.add(item["seq"])

            # æ˜¾ç¤ºæœ€æ–° 5 æ¡ä½œä¸ºåŸºçº¿æ ·æœ¬
            log("")
            log("  ã€åŸºçº¿æ ·æœ¬ - æœ€æ–° 5 æ¡æ–°é—»ã€‘")
            log("-" * 60)
            for i, item in enumerate(all_items[:5], 1):
                log(f"  [{i}]")
                log(format_news_detail(item))
                log("-" * 60)

            # æ˜¾ç¤ºåºå·èŒƒå›´
            seqs = [item['seq'] for item in all_items if item.get('seq')]
            if seqs:
                log(f"  åºå·èŒƒå›´: {min(seqs)} ~ {max(seqs)}")
        else:
            log("  âœ— é¦–æ¬¡è·å–å¤±è´¥")

        log("")
        log(f"åŸºçº¿å»ºç«‹å®Œæˆï¼Œå·²è®°å½• {len(crawler.seen_seqs)} æ¡æ–°é—»")
        log("")
        log("=" * 80)
        log("å¼€å§‹å¢é‡ç›‘æ§...")
        log("=" * 80)
        log("")

        # ========== å¢é‡ç›‘æ§å¾ªç¯ ==========
        start_time = time.time()
        total_new_count = 0
        check_count = 0
        all_new_items = []  # æ”¶é›†æ‰€æœ‰æ–°å¢çš„æ–°é—»

        while time.time() - start_time < duration:
            time.sleep(interval)
            check_count += 1

            elapsed = int(time.time() - start_time)
            remaining = duration - elapsed

            log(f"[{crawler.get_cn_time()}] === ç¬¬ {check_count} æ¬¡æ£€æŸ¥ (å·²è¿è¡Œ {elapsed}s, å‰©ä½™ {remaining}s) ===")

            new_items, all_items, fetch_status, parse_status = crawler.get_incremental_news()

            log(f"  è·å–çŠ¶æ€: {fetch_status}")
            log(f"  è§£æçŠ¶æ€: {parse_status}")
            log(f"  æœ¬æ¬¡è·å–: {len(all_items)} æ¡, å…¶ä¸­æ–°å¢: {len(new_items)} æ¡")
            log(f"  ç´¯è®¡å·²çŸ¥: {len(crawler.seen_seqs)} æ¡")

            if new_items:
                total_new_count += len(new_items)
                all_new_items.extend(new_items)
                log("")
                log(f"  ğŸ†• ã€å‘ç° {len(new_items)} æ¡æ–°æ¶ˆæ¯ã€‘")
                log("-" * 60)
                for i, item in enumerate(new_items, 1):
                    log(f"  [æ–°å¢ {i}]")
                    log(format_news_detail(item))
                    log("-" * 60)
            else:
                log("  æ— æ–°æ¶ˆæ¯")

            log("")

        # ========== æµ‹è¯•å®Œæˆç»Ÿè®¡ ==========
        log("=" * 80)
        log("æµ‹è¯•å®Œæˆ - ç»Ÿè®¡æ‘˜è¦")
        log("=" * 80)
        log(f"ç»“æŸæ—¶é—´: {crawler.get_cn_time()}")
        log(f"å®é™…è¿è¡Œ: {int(time.time() - start_time)} ç§’")
        log(f"æ£€æŸ¥æ¬¡æ•°: {check_count} æ¬¡")
        log(f"æ–°å¢æ¶ˆæ¯: {total_new_count} æ¡")
        log(f"ç´¯è®¡å·²çŸ¥: {len(crawler.seen_seqs)} æ¡")
        log("")

        if all_new_items:
            log("ã€æœ¬æ¬¡æµ‹è¯•æœŸé—´æ‰€æœ‰æ–°å¢æ¶ˆæ¯æ±‡æ€»ã€‘")
            log("-" * 60)
            for i, item in enumerate(all_new_items, 1):
                log(f"[{i}] [{item['pub_date']}] {item['title']}")
                log(f"    é“¾æ¥: {item['url']}")
                log(f"    åºå·: {item['seq']}")
            log("-" * 60)
        else:
            log("æœ¬æ¬¡æµ‹è¯•æœŸé—´æ— æ–°å¢æ¶ˆæ¯")

        log("")
        log("=" * 80)
        log("æ—¥å¿—ç»“æŸ")
        log("=" * 80)

    print(f"\næ—¥å¿—å·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
