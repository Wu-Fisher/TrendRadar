#!/usr/bin/env python3
"""
åŒèŠ±é¡º7x24å°æ—¶è¦é—»ç›´æ’­çˆ¬è™«æµ‹è¯•è„šæœ¬

æ•°æ®æº: http://stock.10jqka.com.cn/thsgd/realtimenews.js
åŠŸèƒ½: æ¯éš”ä¸€å®šæ—¶é—´æŠ“å–æ–°é—»ï¼Œæ£€æµ‹å¹¶è¾“å‡ºå¢é‡æ¶ˆæ¯
"""

import requests
import json
import re
import time
from datetime import datetime
from typing import Optional


class THSNewsCrawler:
    """åŒèŠ±é¡ºå®æ—¶æ–°é—»çˆ¬è™«"""

    DATA_URL = "http://stock.10jqka.com.cn/thsgd/realtimenews.js"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://news.10jqka.com.cn/realtimenews.html",
        })
        # å·²è§è¿‡çš„æ–°é—»åºå·é›†åˆ
        self.seen_seqs: set[int] = set()
        # æœ€æ–°çš„æ–°é—»åºå·
        self.latest_seq: Optional[int] = None

    def fetch_raw_data(self) -> Optional[str]:
        """è·å–åŸå§‹ JS æ•°æ®"""
        try:
            resp = self.session.get(self.DATA_URL, timeout=10)
            resp.encoding = 'gbk'  # åŒèŠ±é¡ºä½¿ç”¨ GBK ç¼–ç 
            return resp.text
        except Exception as e:
            print(f"[ERROR] è·å–æ•°æ®å¤±è´¥: {e}")
            return None

    def parse_jsonp(self, raw_data: str) -> Optional[dict]:
        """è§£æ JSONP æ•°æ® (var thsRss = {...})"""
        try:
            # å»æ‰ var thsRss = å‰ç¼€
            json_str = raw_data.strip()

            # æ‰¾åˆ° JSON å¯¹è±¡çš„èµ·å§‹ä½ç½®
            start = json_str.find('{')
            if start == -1:
                print("[ERROR] æ— æ³•æ‰¾åˆ° JSON èµ·å§‹ä½ç½®")
                return None

            # æ‰¾åˆ° JSON å¯¹è±¡çš„ç»“æŸä½ç½® (æœ€åä¸€ä¸ª }; æˆ– }])
            # å°¾éƒ¨å¯èƒ½æœ‰ JSONP å›è°ƒ: if ( typeof(ths_rss_news_callback) ...
            end = json_str.rfind('};')
            if end != -1:
                json_str = json_str[start:end+1]
            else:
                # å¤‡é€‰ï¼šæ‰¾æœ€åä¸€ä¸ª }
                end = json_str.rfind('}')
                if end != -1:
                    json_str = json_str[start:end+1]
                else:
                    json_str = json_str[start:]

            # å¤–å±‚å±æ€§åæ²¡æœ‰åŒå¼•å·ï¼Œéœ€è¦æ·»åŠ 
            # pubDate, latestNewsSeq, counter, item è¿™äº›å¤–å±‚keyéœ€è¦åŠ å¼•å·
            json_str = re.sub(r'(\{)\s*(pubDate):', r'\1"\2":', json_str)
            json_str = re.sub(r',\s*(latestNewsSeq):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(counter):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(item):', r',"\1":', json_str)

            data = json.loads(json_str)
            return data
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSON è§£æå¤±è´¥: {e}")
            # æ˜¾ç¤ºå‡ºé”™ä½ç½®é™„è¿‘çš„å†…å®¹
            if e.pos:
                print(f"[DEBUG] å‡ºé”™ä½ç½®é™„è¿‘å†…å®¹: ...{json_str[max(0, e.pos-50):e.pos+50]}...")
            return None

    def extract_news_items(self, data: dict) -> list[dict]:
        """æå–æ–°é—»æ¡ç›®"""
        items = data.get("item", [])
        result = []
        for item in items:
            result.append({
                "seq": item.get("seq"),
                "title": item.get("title", "").strip(),
                "content": item.get("content", "").strip(),
                "url": item.get("url", ""),
                "pub_date": item.get("pubDate", ""),
                "source": item.get("source", ""),
                "stocks": item.get("stocks"),
                "stock_code": item.get("stockCode", ""),
            })
        return result

    def get_incremental_news(self) -> list[dict]:
        """è·å–å¢é‡æ–°é—»ï¼ˆåªè¿”å›æ–°å‡ºç°çš„ï¼‰"""
        raw_data = self.fetch_raw_data()
        if not raw_data:
            return []

        data = self.parse_jsonp(raw_data)
        if not data:
            return []

        all_items = self.extract_news_items(data)
        new_items = []

        for item in all_items:
            seq = item.get("seq")
            if seq and seq not in self.seen_seqs:
                self.seen_seqs.add(seq)
                new_items.append(item)

        # æ›´æ–°æœ€æ–°åºå·
        if data.get("latestNewsSeq"):
            self.latest_seq = int(data["latestNewsSeq"])

        return new_items

    def format_news(self, item: dict) -> str:
        """æ ¼å¼åŒ–å•æ¡æ–°é—»è¾“å‡º"""
        lines = []
        lines.append(f"ğŸ“° {item['title']}")
        if item['content']:
            # æˆªå–å†…å®¹æ‘˜è¦
            content = item['content'][:200] + "..." if len(item['content']) > 200 else item['content']
            lines.append(f"   {content}")
        lines.append(f"   ğŸ”— {item['url']}")
        lines.append(f"   â° {item['pub_date']} | æ¥æº: {item['source']}")
        if item['stock_code']:
            lines.append(f"   ğŸ“Š ç›¸å…³è‚¡ç¥¨: {item['stock_code']}")
        return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œ 5 åˆ†é’Ÿçš„å¢é‡ç›‘æ§æµ‹è¯•"""
    print("=" * 60)
    print("åŒèŠ±é¡º 7x24 å°æ—¶è¦é—»ç›´æ’­ - å¢é‡çˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    print(f"æ•°æ®æº: {THSNewsCrawler.DATA_URL}")
    print(f"æµ‹è¯•æ—¶é•¿: 5 åˆ†é’Ÿ")
    print(f"æ£€æŸ¥é—´éš”: 10 ç§’")
    print("=" * 60)

    crawler = THSNewsCrawler()

    # ç¬¬ä¸€æ¬¡è·å–ï¼Œå»ºç«‹åŸºçº¿
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] é¦–æ¬¡è·å–æ•°æ®ï¼Œå»ºç«‹åŸºçº¿...")
    raw_data = crawler.fetch_raw_data()
    if raw_data:
        data = crawler.parse_jsonp(raw_data)
        if data:
            items = crawler.extract_news_items(data)
            print(f"  âœ“ è·å–åˆ° {len(items)} æ¡æ–°é—»")
            print(f"  âœ“ æœ€æ–°åºå·: {data.get('latestNewsSeq')}")
            print(f"  âœ“ å‘å¸ƒæ—¶é—´: {data.get('pubDate')}")

            # å°†æ‰€æœ‰å·²æœ‰æ–°é—»æ ‡è®°ä¸ºå·²è§
            for item in items:
                if item.get("seq"):
                    crawler.seen_seqs.add(item["seq"])

            # æ˜¾ç¤ºæœ€æ–° 3 æ¡ä½œä¸ºæ ·ä¾‹
            print("\n  æœ€æ–° 3 æ¡æ–°é—»æ ·ä¾‹:")
            print("-" * 50)
            for item in items[:3]:
                print(crawler.format_news(item))
                print("-" * 50)

    print(f"\nåŸºçº¿å»ºç«‹å®Œæˆï¼Œå·²è®°å½• {len(crawler.seen_seqs)} æ¡æ–°é—»")
    print("å¼€å§‹å¢é‡ç›‘æ§...\n")

    # å¢é‡ç›‘æ§å¾ªç¯
    start_time = time.time()
    duration = 5 * 60  # 5 åˆ†é’Ÿ
    interval = 10  # 10 ç§’æ£€æŸ¥ä¸€æ¬¡
    new_count = 0
    check_count = 0

    try:
        while time.time() - start_time < duration:
            time.sleep(interval)
            check_count += 1

            elapsed = int(time.time() - start_time)
            remaining = duration - elapsed

            new_items = crawler.get_incremental_news()

            if new_items:
                new_count += len(new_items)
                print(f"\nğŸ†• [{datetime.now().strftime('%H:%M:%S')}] å‘ç° {len(new_items)} æ¡æ–°æ¶ˆæ¯!")
                print("-" * 50)
                for item in new_items:
                    print(crawler.format_news(item))
                    print("-" * 50)
            else:
                # ç®€å•çš„è¿›åº¦æç¤º
                print(f"[{datetime.now().strftime('%H:%M:%S')}] æ£€æŸ¥ #{check_count} - æ— æ–°æ¶ˆæ¯ (å‰©ä½™ {remaining}s)", end="\r")

    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")

    # ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)
    print(f"è¿è¡Œæ—¶é•¿: {int(time.time() - start_time)} ç§’")
    print(f"æ£€æŸ¥æ¬¡æ•°: {check_count}")
    print(f"æ–°å¢æ¶ˆæ¯: {new_count} æ¡")
    print(f"å·²çŸ¥æ¶ˆæ¯: {len(crawler.seen_seqs)} æ¡")


if __name__ == "__main__":
    main()
