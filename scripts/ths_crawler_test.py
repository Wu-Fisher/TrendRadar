#!/usr/bin/env python3
"""
åŒèŠ±é¡º7x24å°æ—¶è¦é—»ç›´æ’­çˆ¬è™« - æ”¹è¿›ç‰ˆè¯¦ç»†æ—¥å¿—æµ‹è¯•
ä¿®å¤ï¼šæ·»åŠ ç¼“å­˜ç»•è¿‡æœºåˆ¶
åŠŸèƒ½ï¼šè·å–æ–°é—»åˆ—è¡¨ + æŠ“å–æ¯æ¡æ–°é—»çš„å®Œæ•´å†…å®¹
è¾“å‡ºï¼šè¯¦ç»†æ—¥å¿— + å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºä¸åŸç½‘é¡µå¯¹æ¯”ï¼‰
"""

import requests
import json
import re
import time
from datetime import datetime
from typing import Optional
import pytz

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    print("è­¦å‘Š: æœªå®‰è£… beautifulsoup4ï¼Œæ— æ³•è·å–æ–°é—»å®Œæ•´å†…å®¹")
    print("å®‰è£…å‘½ä»¤: pip install beautifulsoup4")


class THSNewsCrawler:
    """åŒèŠ±é¡ºå®æ—¶æ–°é—»çˆ¬è™«ï¼ˆå¸¦ç¼“å­˜ç»•è¿‡ï¼‰"""

    BASE_URL = "http://stock.10jqka.com.cn/thsgd/realtimenews.js"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://news.10jqka.com.cn/realtimenews.html",
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Pragma": "no-cache",
            "Expires": "0",
        })
        self.seen_seqs: set[int] = set()
        self.all_news: dict[int, dict] = {}  # seq -> news itemï¼Œæ”¶é›†æ‰€æœ‰æ–°é—»
        self.tz = pytz.timezone('Asia/Shanghai')

    def get_cn_time(self) -> str:
        """è·å–ä¸­å›½æ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.now(self.tz).strftime('%Y-%m-%d %H:%M:%S')

    def fetch_raw_data(self) -> tuple[Optional[str], str]:
        """è·å–åŸå§‹æ•°æ®ï¼ˆå¸¦ç¼“å­˜ç»•è¿‡ï¼‰"""
        try:
            # æ·»åŠ æ—¶é—´æˆ³å‚æ•°ç»•è¿‡ç¼“å­˜
            url = f"{self.BASE_URL}?v={int(time.time() * 1000)}"
            resp = self.session.get(url, timeout=10)
            resp.encoding = 'gbk'
            return resp.text, f"HTTP {resp.status_code}, {len(resp.text)} bytes"
        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError as e:
            return None, f"è¿æ¥é”™è¯¯: {e}"
        except Exception as e:
            return None, f"æœªçŸ¥é”™è¯¯: {e}"

    def fetch_full_content(self, url: str) -> tuple[Optional[str], str]:
        """è·å–æ–°é—»è¯¦æƒ…é¡µçš„å®Œæ•´å†…å®¹"""
        if not HAS_BS4:
            return None, "æœªå®‰è£… beautifulsoup4"

        if not url:
            return None, "URL ä¸ºç©º"

        # å°è¯•çš„ URL åˆ—è¡¨ï¼ˆåŸå§‹ URL + å¯èƒ½çš„å¤‡é€‰ URLï¼‰
        urls_to_try = [url]

        # å¦‚æœæ˜¯ news.10jqka.com.cn åŸŸåï¼ˆNext.js SPAï¼Œæ— æ³•ç›´æ¥è§£æï¼‰
        # å°è¯•è½¬æ¢ä¸º stock.10jqka.com.cn
        if 'news.10jqka.com.cn' in url:
            alt_url = url.replace('news.10jqka.com.cn', 'stock.10jqka.com.cn')
            urls_to_try.append(alt_url)

        last_error = ""
        for try_url in urls_to_try:
            content, status = self._fetch_content_from_url(try_url)
            if content:
                return content, status
            last_error = status

        return None, last_error

    def _fetch_content_from_url(self, url: str) -> tuple[Optional[str], str]:
        """ä»æŒ‡å®š URL è·å–å†…å®¹çš„å†…éƒ¨æ–¹æ³•"""
        try:
            resp = self.session.get(url, timeout=10)
            resp.encoding = 'gbk'

            soup = BeautifulSoup(resp.text, 'html.parser')

            # æ‰¾æ­£æ–‡å®¹å™¨ (class="main-text" æˆ– class="atc-content")
            content_div = soup.find('div', class_='main-text')
            if not content_div:
                content_div = soup.find('div', class_='atc-content')

            if not content_div:
                return None, "æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨"

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for tag in content_div.find_all(['script', 'style']):
                tag.decompose()

            # æå–æ‰€æœ‰ p æ ‡ç­¾çš„æ–‡æœ¬
            paragraphs = content_div.find_all('p')
            if paragraphs:
                # è¿‡æ»¤æ‰å¹¿å‘Šå’Œæ— å…³å†…å®¹
                texts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    # è·³è¿‡å¸¸è§çš„å¹¿å‘Š/æ— å…³æ–‡æœ¬
                    if text and not text.startswith('å…³æ³¨åŒèŠ±é¡ºè´¢ç»') and len(text) > 5:
                        texts.append(text)
                if texts:
                    # å»é‡ï¼ˆæœ‰æ—¶å†…å®¹ä¼šé‡å¤ï¼‰
                    seen = set()
                    unique_texts = []
                    for t in texts:
                        if t not in seen:
                            seen.add(t)
                            unique_texts.append(t)
                    full_text = '\n'.join(unique_texts)
                    return full_text, "æˆåŠŸ"

            # å¦‚æœæ²¡æœ‰ p æ ‡ç­¾ï¼Œç›´æ¥æå–æ–‡æœ¬
            text = content_div.get_text(separator='\n', strip=True)
            if text:
                return text, "æˆåŠŸ(ç›´æ¥æå–)"

            return None, "æ­£æ–‡ä¸ºç©º"

        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError as e:
            return None, f"è¿æ¥é”™è¯¯"
        except Exception as e:
            return None, f"é”™è¯¯: {e}"

    def parse_jsonp(self, raw_data: str) -> tuple[Optional[dict], str]:
        """è§£æ JSONP æ•°æ®"""
        try:
            json_str = raw_data.strip()

            start = json_str.find('{')
            if start == -1:
                return None, "æ— æ³•æ‰¾åˆ° JSON èµ·å§‹ä½ç½®"

            end = json_str.rfind('};')
            if end != -1:
                json_str = json_str[start:end+1]
            else:
                end = json_str.rfind('}')
                if end != -1:
                    json_str = json_str[start:end+1]
                else:
                    json_str = json_str[start:]

            # ç»™å¤–å±‚å±æ€§ååŠ åŒå¼•å·
            json_str = re.sub(r'(\{)\s*(pubDate):', r'\1"\2":', json_str)
            json_str = re.sub(r',\s*(latestNewsSeq):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(counter):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(item):', r',"\1":', json_str)

            data = json.loads(json_str)
            item_count = len(data.get("item", []))
            return data, f"è§£ææˆåŠŸ, {item_count} æ¡, æ•°æ®æ—¶é—´: {data.get('pubDate')}, æœ€æ–°åºå·: {data.get('latestNewsSeq')}"
        except json.JSONDecodeError as e:
            return None, f"JSON è§£æå¤±è´¥: {e}"

    def extract_news_item(self, item: dict) -> dict:
        """æå–å¹¶å¤„ç†å•æ¡æ–°é—»"""
        return {
            "seq": item.get("seq"),
            "title": item.get("title", "").strip(),
            "content": item.get("content", "").strip(),  # æ‘˜è¦
            "full_content": "",  # å®Œæ•´å†…å®¹ï¼ˆéœ€è¦å•ç‹¬è·å–ï¼‰
            "url": item.get("url", ""),
            "pub_date": item.get("pubDate", ""),
            "source": item.get("source", ""),
            "stocks": item.get("stocks"),
            "stock_code": item.get("stockCode", ""),
            "category": item.get("category", ""),
            "implevel": item.get("implevel", ""),
        }

    def get_incremental_news(self) -> tuple[list[dict], list[dict], str, str, str]:
        """è·å–å¢é‡æ–°é—»
        è¿”å›: (æ–°å¢æ–°é—», å…¨éƒ¨æ–°é—», è·å–çŠ¶æ€, è§£æçŠ¶æ€, æ•°æ®å‘å¸ƒæ—¶é—´)
        """
        raw_data, fetch_status = self.fetch_raw_data()
        if not raw_data:
            return [], [], fetch_status, "æœªæ‰§è¡Œ", ""

        data, parse_status = self.parse_jsonp(raw_data)
        if not data:
            return [], [], fetch_status, parse_status, ""

        data_pub_time = data.get("pubDate", "")
        all_items = [self.extract_news_item(item) for item in data.get("item", [])]

        new_items = []
        for item in all_items:
            seq = item.get("seq")
            if seq:
                # æ”¶é›†åˆ°æ€»åº“
                if seq not in self.all_news:
                    self.all_news[seq] = item

                # æ£€æŸ¥æ˜¯å¦æ–°å¢
                if seq not in self.seen_seqs:
                    self.seen_seqs.add(seq)
                    new_items.append(item)

        return new_items, all_items, fetch_status, parse_status, data_pub_time

    def fetch_all_full_contents(self, delay: float = 0.3, log_func=None) -> dict:
        """æ‰¹é‡è·å–æ‰€æœ‰æ–°é—»çš„å®Œæ•´å†…å®¹

        Args:
            delay: æ¯æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            log_func: æ—¥å¿—è¾“å‡ºå‡½æ•°ï¼Œå¯é€‰

        Returns:
            ç»Ÿè®¡ä¿¡æ¯ {"success": æˆåŠŸæ•°, "failed": å¤±è´¥æ•°, "skipped": è·³è¿‡æ•°}
        """
        if not HAS_BS4:
            if log_func:
                log_func("  âš ï¸ æœªå®‰è£… beautifulsoup4ï¼Œè·³è¿‡è·å–å®Œæ•´å†…å®¹")
            return {"success": 0, "failed": 0, "skipped": len(self.all_news)}

        stats = {"success": 0, "failed": 0, "skipped": 0}
        total = len(self.all_news)

        for i, (seq, item) in enumerate(self.all_news.items(), 1):
            # å¦‚æœå·²ç»æœ‰å®Œæ•´å†…å®¹ï¼Œè·³è¿‡
            if item.get("full_content"):
                stats["skipped"] += 1
                continue

            url = item.get("url", "")
            if not url:
                stats["skipped"] += 1
                continue

            if log_func:
                log_func(f"  [{i}/{total}] è·å– seq={seq} ...", end="")

            full_content, status = self.fetch_full_content(url)

            if full_content:
                item["full_content"] = full_content
                stats["success"] += 1
                if log_func:
                    log_func(f" âœ“ ({len(full_content)}å­—)")
            else:
                stats["failed"] += 1
                if log_func:
                    log_func(f" âœ— ({status})")

            # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < total:
                time.sleep(delay)

        return stats


def format_news_detail(item: dict, indent: str = "    ") -> str:
    """æ ¼å¼åŒ–å•æ¡æ–°é—»çš„è¯¦ç»†ä¿¡æ¯"""
    lines = []
    lines.append(f"{indent}åºå·: {item['seq']}")
    lines.append(f"{indent}æ ‡é¢˜: {item['title']}")
    lines.append(f"{indent}å‘å¸ƒæ—¶é—´: {item['pub_date']}")
    lines.append(f"{indent}æ¥æº: {item['source']}")
    lines.append(f"{indent}é“¾æ¥: {item['url']}")
    if item['stock_code']:
        lines.append(f"{indent}ç›¸å…³è‚¡ç¥¨: {item['stock_code']}")
    if item['implevel']:
        lines.append(f"{indent}é‡è¦ç¨‹åº¦: {item['implevel']}")
    content = item['content']
    if len(content) > 200:
        content = content[:200] + "..."
    lines.append(f"{indent}å†…å®¹æ‘˜è¦: {content}")
    return "\n".join(lines)


def format_news_simple(item: dict) -> str:
    """æ ¼å¼åŒ–å•æ¡æ–°é—»çš„ç®€æ´ä¿¡æ¯ï¼ˆç”¨äºå¯¹æ¯”åˆ—è¡¨ï¼‰"""
    stock_info = f" [è‚¡ç¥¨:{item['stock_code']}]" if item['stock_code'] else ""
    return f"[{item['pub_date']}] seq={item['seq']}{stock_info} {item['title']}\n  é“¾æ¥: {item['url']}"


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    duration = 10 * 60  # 10 åˆ†é’Ÿ
    interval = 30  # 30 ç§’é—´éš”

    # è¾“å‡ºæ–‡ä»¶
    log_file = "/home/wufisher/ws/dev/TrendRadar/output/ths_crawler_test_log_v2.txt"
    news_list_file = "/home/wufisher/ws/dev/TrendRadar/output/ths_crawler_news_list.txt"

    crawler = THSNewsCrawler()

    with open(log_file, 'w', encoding='utf-8') as f:
        def log(msg: str):
            print(msg, flush=True)
            f.write(msg + "\n")
            f.flush()

        log("=" * 80)
        log("åŒèŠ±é¡º 7x24 å°æ—¶è¦é—»ç›´æ’­ - æ”¹è¿›ç‰ˆè¯¦ç»†æ—¥å¿—æµ‹è¯•")
        log("=" * 80)
        log(f"å¼€å§‹æ—¶é—´: {crawler.get_cn_time()}")
        log(f"æµ‹è¯•æ—¶é•¿: {duration // 60} åˆ†é’Ÿ")
        log(f"æ£€æŸ¥é—´éš”: {interval} ç§’")
        log(f"æ•°æ®æº: {THSNewsCrawler.BASE_URL}")
        log(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        log(f"æ¶ˆæ¯åˆ—è¡¨: {news_list_file}")
        log("æ”¹è¿›: æ·»åŠ ç¼“å­˜ç»•è¿‡ï¼ˆæ—¶é—´æˆ³å‚æ•° + Cache-Control å¤´ï¼‰")
        log("=" * 80)
        log("")

        # ========== é¦–æ¬¡è·å– ==========
        log(f"[{crawler.get_cn_time()}] === é¦–æ¬¡è·å–ï¼ˆå»ºç«‹åŸºçº¿ï¼‰===")
        new_items, all_items, fetch_status, parse_status, data_time = crawler.get_incremental_news()
        log(f"  è·å–çŠ¶æ€: {fetch_status}")
        log(f"  è§£æçŠ¶æ€: {parse_status}")

        if all_items:
            log(f"  è·å–åˆ° {len(all_items)} æ¡æ–°é—»ï¼ˆå…¨éƒ¨æ ‡è®°ä¸ºå·²çŸ¥ï¼‰")

            # æ˜¾ç¤ºæœ€æ–° 5 æ¡
            log("")
            log("  ã€åŸºçº¿æ ·æœ¬ - æœ€æ–° 5 æ¡æ–°é—»ã€‘")
            log("-" * 60)
            for i, item in enumerate(all_items[:5], 1):
                log(f"  [{i}]")
                log(format_news_detail(item))
                log("-" * 60)

            seqs = [item['seq'] for item in all_items if item.get('seq')]
            if seqs:
                log(f"  åºå·èŒƒå›´: {min(seqs)} ~ {max(seqs)}")
        else:
            log("  âœ— é¦–æ¬¡è·å–å¤±è´¥")

        log("")
        log(f"åŸºçº¿å»ºç«‹å®Œæˆï¼Œå·²è®°å½• {len(crawler.seen_seqs)} æ¡æ–°é—»")
        log("")
        log("=" * 80)
        log("å¼€å§‹å¢é‡ç›‘æ§...")
        log("=" * 80)
        log("")

        # ========== å¢é‡ç›‘æ§ ==========
        start_time = time.time()
        total_new_count = 0
        check_count = 0
        incremental_news = []  # æµ‹è¯•æœŸé—´æ–°å¢çš„æ¶ˆæ¯

        while time.time() - start_time < duration:
            time.sleep(interval)
            check_count += 1

            elapsed = int(time.time() - start_time)
            remaining = duration - elapsed

            log(f"[{crawler.get_cn_time()}] === ç¬¬ {check_count} æ¬¡æ£€æŸ¥ (å·²è¿è¡Œ {elapsed}s, å‰©ä½™ {remaining}s) ===")

            new_items, all_items, fetch_status, parse_status, data_time = crawler.get_incremental_news()

            log(f"  è·å–çŠ¶æ€: {fetch_status}")
            log(f"  è§£æçŠ¶æ€: {parse_status}")
            log(f"  æœ¬æ¬¡è·å–: {len(all_items)} æ¡, æ–°å¢: {len(new_items)} æ¡")
            log(f"  ç´¯è®¡å·²çŸ¥: {len(crawler.seen_seqs)} æ¡, æ€»åº“: {len(crawler.all_news)} æ¡")

            if new_items:
                total_new_count += len(new_items)
                incremental_news.extend(new_items)
                log("")
                log(f"  ğŸ†• ã€å‘ç° {len(new_items)} æ¡æ–°æ¶ˆæ¯ã€‘")
                log("-" * 60)
                for i, item in enumerate(new_items, 1):
                    log(f"  [æ–°å¢ {i}]")
                    log(format_news_detail(item))
                    log("-" * 60)
            else:
                log("  æ— æ–°æ¶ˆæ¯")

            log("")

        # ========== ç»Ÿè®¡ ==========
        log("=" * 80)
        log("æµ‹è¯•å®Œæˆ - ç»Ÿè®¡æ‘˜è¦")
        log("=" * 80)
        log(f"ç»“æŸæ—¶é—´: {crawler.get_cn_time()}")
        log(f"å®é™…è¿è¡Œ: {int(time.time() - start_time)} ç§’")
        log(f"æ£€æŸ¥æ¬¡æ•°: {check_count} æ¬¡")
        log(f"æ–°å¢æ¶ˆæ¯: {total_new_count} æ¡")
        log(f"æ€»æ”¶é›†: {len(crawler.all_news)} æ¡ä¸é‡å¤æ–°é—»")
        log("")

        if incremental_news:
            log("ã€æµ‹è¯•æœŸé—´æ–°å¢æ¶ˆæ¯æ±‡æ€»ã€‘")
            log("-" * 60)
            for i, item in enumerate(incremental_news, 1):
                log(f"[{i}] {format_news_simple(item)}")
            log("-" * 60)
        else:
            log("æµ‹è¯•æœŸé—´æ— æ–°å¢æ¶ˆæ¯")

        log("")

        # ========== è·å–å®Œæ•´å†…å®¹ ==========
        log("=" * 80)
        log("å¼€å§‹è·å–æ–°é—»å®Œæ•´å†…å®¹...")
        log("=" * 80)

        def log_inline(msg: str, end: str = "\n"):
            """æ”¯æŒä¸æ¢è¡Œçš„æ—¥å¿—"""
            print(msg, end=end, flush=True)
            f.write(msg + end)
            f.flush()

        stats = crawler.fetch_all_full_contents(delay=0.3, log_func=log_inline)
        log("")
        log(f"è·å–å®Œæˆ: æˆåŠŸ {stats['success']} æ¡, å¤±è´¥ {stats['failed']} æ¡, è·³è¿‡ {stats['skipped']} æ¡")

        log("")
        log("=" * 80)
        log(f"å®Œæ•´æ¶ˆæ¯åˆ—è¡¨å·²è¾“å‡ºåˆ°: {news_list_file}")
        log("=" * 80)

    # ========== è¾“å‡ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºä¸åŸç½‘é¡µå¯¹æ¯”ï¼‰==========
    with open(news_list_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("åŒèŠ±é¡º 7x24 å°æ—¶è¦é—» - å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºä¸åŸç½‘é¡µå¯¹æ¯”ï¼‰\n")
        f.write("=" * 80 + "\n")
        f.write(f"å¯¼å‡ºæ—¶é—´: {crawler.get_cn_time()}\n")
        f.write(f"æ€»æ¡æ•°: {len(crawler.all_news)} æ¡\n")
        f.write("æ’åº: æŒ‰åºå·ä»å¤§åˆ°å°ï¼ˆæœ€æ–°åœ¨å‰ï¼‰\n")
        f.write("=" * 80 + "\n\n")

        # æŒ‰åºå·æ’åºï¼ˆä»å¤§åˆ°å° = æœ€æ–°åœ¨å‰ï¼‰
        sorted_news = sorted(crawler.all_news.values(), key=lambda x: x.get('seq', 0), reverse=True)

        for i, item in enumerate(sorted_news, 1):
            f.write(f"[{i:03d}] ----------------------------------------\n")
            f.write(f"åºå·: {item['seq']}\n")
            f.write(f"æ—¶é—´: {item['pub_date']}\n")
            f.write(f"æ ‡é¢˜: {item['title']}\n")
            f.write(f"é“¾æ¥: {item['url']}\n")
            if item['stock_code']:
                f.write(f"è‚¡ç¥¨: {item['stock_code']}\n")
            if item['implevel']:
                f.write(f"é‡è¦: {item['implevel']}\n")
            # ä¼˜å…ˆä½¿ç”¨å®Œæ•´å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ‘˜è¦
            content = item.get('full_content') or item.get('content', '')
            f.write(f"å†…å®¹: {content}\n")
            f.write("\n")

        f.write("=" * 80 + "\n")
        f.write("åˆ—è¡¨ç»“æŸ\n")
        f.write("=" * 80 + "\n")

    print(f"\næ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    print(f"æ¶ˆæ¯åˆ—è¡¨å·²ä¿å­˜åˆ°: {news_list_file}")


if __name__ == "__main__":
    main()
