#!/usr/bin/env python3
"""
åŒèŠ±é¡º7x24å°æ—¶è¦é—»ç›´æ’­çˆ¬è™« - æ”¹è¿›ç‰ˆè¯¦ç»†æ—¥å¿—æµ‹è¯•
ä¿®å¤ï¼šæ·»åŠ ç¼“å­˜ç»•è¿‡æœºåˆ¶
è¾“å‡ºï¼šè¯¦ç»†æ—¥å¿— + å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºä¸åŸç½‘é¡µå¯¹æ¯”ï¼‰
"""

import requests
import json
import re
import time
from datetime import datetime
from typing import Optional
import pytz


class THSNewsCrawler:
    """åŒèŠ±é¡ºå®æ—¶æ–°é—»çˆ¬è™«ï¼ˆå¸¦ç¼“å­˜ç»•è¿‡ï¼‰"""

    BASE_URL = "http://stock.10jqka.com.cn/thsgd/realtimenews.js"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://news.10jqka.com.cn/realtimenews.html",
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Pragma": "no-cache",
            "Expires": "0",
        })
        self.seen_seqs: set[int] = set()
        self.all_news: dict[int, dict] = {}  # seq -> news itemï¼Œæ”¶é›†æ‰€æœ‰æ–°é—»
        self.tz = pytz.timezone('Asia/Shanghai')

    def get_cn_time(self) -> str:
        """è·å–ä¸­å›½æ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.now(self.tz).strftime('%Y-%m-%d %H:%M:%S')

    def fetch_raw_data(self) -> tuple[Optional[str], str]:
        """è·å–åŸå§‹æ•°æ®ï¼ˆå¸¦ç¼“å­˜ç»•è¿‡ï¼‰"""
        try:
            # æ·»åŠ æ—¶é—´æˆ³å‚æ•°ç»•è¿‡ç¼“å­˜
            url = f"{self.BASE_URL}?v={int(time.time() * 1000)}"
            resp = self.session.get(url, timeout=10)
            resp.encoding = 'gbk'
            return resp.text, f"HTTP {resp.status_code}, {len(resp.text)} bytes"
        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError as e:
            return None, f"è¿æ¥é”™è¯¯: {e}"
        except Exception as e:
            return None, f"æœªçŸ¥é”™è¯¯: {e}"

    def parse_jsonp(self, raw_data: str) -> tuple[Optional[dict], str]:
        """è§£æ JSONP æ•°æ®"""
        try:
            json_str = raw_data.strip()

            start = json_str.find('{')
            if start == -1:
                return None, "æ— æ³•æ‰¾åˆ° JSON èµ·å§‹ä½ç½®"

            end = json_str.rfind('};')
            if end != -1:
                json_str = json_str[start:end+1]
            else:
                end = json_str.rfind('}')
                if end != -1:
                    json_str = json_str[start:end+1]
                else:
                    json_str = json_str[start:]

            # ç»™å¤–å±‚å±æ€§ååŠ åŒå¼•å·
            json_str = re.sub(r'(\{)\s*(pubDate):', r'\1"\2":', json_str)
            json_str = re.sub(r',\s*(latestNewsSeq):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(counter):', r',"\1":', json_str)
            json_str = re.sub(r',\s*(item):', r',"\1":', json_str)

            data = json.loads(json_str)
            item_count = len(data.get("item", []))
            return data, f"è§£ææˆåŠŸ, {item_count} æ¡, æ•°æ®æ—¶é—´: {data.get('pubDate')}, æœ€æ–°åºå·: {data.get('latestNewsSeq')}"
        except json.JSONDecodeError as e:
            return None, f"JSON è§£æå¤±è´¥: {e}"

    def extract_news_item(self, item: dict) -> dict:
        """æå–å¹¶å¤„ç†å•æ¡æ–°é—»"""
        return {
            "seq": item.get("seq"),
            "title": item.get("title", "").strip(),
            "content": item.get("content", "").strip(),
            "url": item.get("url", ""),
            "pub_date": item.get("pubDate", ""),
            "source": item.get("source", ""),
            "stocks": item.get("stocks"),
            "stock_code": item.get("stockCode", ""),
            "category": item.get("category", ""),
            "implevel": item.get("implevel", ""),
        }

    def get_incremental_news(self) -> tuple[list[dict], list[dict], str, str, str]:
        """è·å–å¢é‡æ–°é—»
        è¿”å›: (æ–°å¢æ–°é—», å…¨éƒ¨æ–°é—», è·å–çŠ¶æ€, è§£æçŠ¶æ€, æ•°æ®å‘å¸ƒæ—¶é—´)
        """
        raw_data, fetch_status = self.fetch_raw_data()
        if not raw_data:
            return [], [], fetch_status, "æœªæ‰§è¡Œ", ""

        data, parse_status = self.parse_jsonp(raw_data)
        if not data:
            return [], [], fetch_status, parse_status, ""

        data_pub_time = data.get("pubDate", "")
        all_items = [self.extract_news_item(item) for item in data.get("item", [])]

        new_items = []
        for item in all_items:
            seq = item.get("seq")
            if seq:
                # æ”¶é›†åˆ°æ€»åº“
                if seq not in self.all_news:
                    self.all_news[seq] = item

                # æ£€æŸ¥æ˜¯å¦æ–°å¢
                if seq not in self.seen_seqs:
                    self.seen_seqs.add(seq)
                    new_items.append(item)

        return new_items, all_items, fetch_status, parse_status, data_pub_time


def format_news_detail(item: dict, indent: str = "    ") -> str:
    """æ ¼å¼åŒ–å•æ¡æ–°é—»çš„è¯¦ç»†ä¿¡æ¯"""
    lines = []
    lines.append(f"{indent}åºå·: {item['seq']}")
    lines.append(f"{indent}æ ‡é¢˜: {item['title']}")
    lines.append(f"{indent}å‘å¸ƒæ—¶é—´: {item['pub_date']}")
    lines.append(f"{indent}æ¥æº: {item['source']}")
    lines.append(f"{indent}é“¾æ¥: {item['url']}")
    if item['stock_code']:
        lines.append(f"{indent}ç›¸å…³è‚¡ç¥¨: {item['stock_code']}")
    if item['implevel']:
        lines.append(f"{indent}é‡è¦ç¨‹åº¦: {item['implevel']}")
    content = item['content']
    if len(content) > 200:
        content = content[:200] + "..."
    lines.append(f"{indent}å†…å®¹æ‘˜è¦: {content}")
    return "\n".join(lines)


def format_news_simple(item: dict) -> str:
    """æ ¼å¼åŒ–å•æ¡æ–°é—»çš„ç®€æ´ä¿¡æ¯ï¼ˆç”¨äºå¯¹æ¯”åˆ—è¡¨ï¼‰"""
    stock_info = f" [è‚¡ç¥¨:{item['stock_code']}]" if item['stock_code'] else ""
    return f"[{item['pub_date']}] seq={item['seq']}{stock_info} {item['title']}\n  é“¾æ¥: {item['url']}"


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    duration = 10 * 60  # 10 åˆ†é’Ÿ
    interval = 30  # 30 ç§’é—´éš”

    # è¾“å‡ºæ–‡ä»¶
    log_file = "/home/wufisher/ws/dev/TrendRadar/output/ths_crawler_test_log_v2.txt"
    news_list_file = "/home/wufisher/ws/dev/TrendRadar/output/ths_crawler_news_list.txt"

    crawler = THSNewsCrawler()

    with open(log_file, 'w', encoding='utf-8') as f:
        def log(msg: str):
            print(msg, flush=True)
            f.write(msg + "\n")
            f.flush()

        log("=" * 80)
        log("åŒèŠ±é¡º 7x24 å°æ—¶è¦é—»ç›´æ’­ - æ”¹è¿›ç‰ˆè¯¦ç»†æ—¥å¿—æµ‹è¯•")
        log("=" * 80)
        log(f"å¼€å§‹æ—¶é—´: {crawler.get_cn_time()}")
        log(f"æµ‹è¯•æ—¶é•¿: {duration // 60} åˆ†é’Ÿ")
        log(f"æ£€æŸ¥é—´éš”: {interval} ç§’")
        log(f"æ•°æ®æº: {THSNewsCrawler.BASE_URL}")
        log(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        log(f"æ¶ˆæ¯åˆ—è¡¨: {news_list_file}")
        log("æ”¹è¿›: æ·»åŠ ç¼“å­˜ç»•è¿‡ï¼ˆæ—¶é—´æˆ³å‚æ•° + Cache-Control å¤´ï¼‰")
        log("=" * 80)
        log("")

        # ========== é¦–æ¬¡è·å– ==========
        log(f"[{crawler.get_cn_time()}] === é¦–æ¬¡è·å–ï¼ˆå»ºç«‹åŸºçº¿ï¼‰===")
        new_items, all_items, fetch_status, parse_status, data_time = crawler.get_incremental_news()
        log(f"  è·å–çŠ¶æ€: {fetch_status}")
        log(f"  è§£æçŠ¶æ€: {parse_status}")

        if all_items:
            log(f"  è·å–åˆ° {len(all_items)} æ¡æ–°é—»ï¼ˆå…¨éƒ¨æ ‡è®°ä¸ºå·²çŸ¥ï¼‰")

            # æ˜¾ç¤ºæœ€æ–° 5 æ¡
            log("")
            log("  ã€åŸºçº¿æ ·æœ¬ - æœ€æ–° 5 æ¡æ–°é—»ã€‘")
            log("-" * 60)
            for i, item in enumerate(all_items[:5], 1):
                log(f"  [{i}]")
                log(format_news_detail(item))
                log("-" * 60)

            seqs = [item['seq'] for item in all_items if item.get('seq')]
            if seqs:
                log(f"  åºå·èŒƒå›´: {min(seqs)} ~ {max(seqs)}")
        else:
            log("  âœ— é¦–æ¬¡è·å–å¤±è´¥")

        log("")
        log(f"åŸºçº¿å»ºç«‹å®Œæˆï¼Œå·²è®°å½• {len(crawler.seen_seqs)} æ¡æ–°é—»")
        log("")
        log("=" * 80)
        log("å¼€å§‹å¢é‡ç›‘æ§...")
        log("=" * 80)
        log("")

        # ========== å¢é‡ç›‘æ§ ==========
        start_time = time.time()
        total_new_count = 0
        check_count = 0
        incremental_news = []  # æµ‹è¯•æœŸé—´æ–°å¢çš„æ¶ˆæ¯

        while time.time() - start_time < duration:
            time.sleep(interval)
            check_count += 1

            elapsed = int(time.time() - start_time)
            remaining = duration - elapsed

            log(f"[{crawler.get_cn_time()}] === ç¬¬ {check_count} æ¬¡æ£€æŸ¥ (å·²è¿è¡Œ {elapsed}s, å‰©ä½™ {remaining}s) ===")

            new_items, all_items, fetch_status, parse_status, data_time = crawler.get_incremental_news()

            log(f"  è·å–çŠ¶æ€: {fetch_status}")
            log(f"  è§£æçŠ¶æ€: {parse_status}")
            log(f"  æœ¬æ¬¡è·å–: {len(all_items)} æ¡, æ–°å¢: {len(new_items)} æ¡")
            log(f"  ç´¯è®¡å·²çŸ¥: {len(crawler.seen_seqs)} æ¡, æ€»åº“: {len(crawler.all_news)} æ¡")

            if new_items:
                total_new_count += len(new_items)
                incremental_news.extend(new_items)
                log("")
                log(f"  ğŸ†• ã€å‘ç° {len(new_items)} æ¡æ–°æ¶ˆæ¯ã€‘")
                log("-" * 60)
                for i, item in enumerate(new_items, 1):
                    log(f"  [æ–°å¢ {i}]")
                    log(format_news_detail(item))
                    log("-" * 60)
            else:
                log("  æ— æ–°æ¶ˆæ¯")

            log("")

        # ========== ç»Ÿè®¡ ==========
        log("=" * 80)
        log("æµ‹è¯•å®Œæˆ - ç»Ÿè®¡æ‘˜è¦")
        log("=" * 80)
        log(f"ç»“æŸæ—¶é—´: {crawler.get_cn_time()}")
        log(f"å®é™…è¿è¡Œ: {int(time.time() - start_time)} ç§’")
        log(f"æ£€æŸ¥æ¬¡æ•°: {check_count} æ¬¡")
        log(f"æ–°å¢æ¶ˆæ¯: {total_new_count} æ¡")
        log(f"æ€»æ”¶é›†: {len(crawler.all_news)} æ¡ä¸é‡å¤æ–°é—»")
        log("")

        if incremental_news:
            log("ã€æµ‹è¯•æœŸé—´æ–°å¢æ¶ˆæ¯æ±‡æ€»ã€‘")
            log("-" * 60)
            for i, item in enumerate(incremental_news, 1):
                log(f"[{i}] {format_news_simple(item)}")
            log("-" * 60)
        else:
            log("æµ‹è¯•æœŸé—´æ— æ–°å¢æ¶ˆæ¯")

        log("")
        log("=" * 80)
        log(f"å®Œæ•´æ¶ˆæ¯åˆ—è¡¨å·²è¾“å‡ºåˆ°: {news_list_file}")
        log("=" * 80)

    # ========== è¾“å‡ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºä¸åŸç½‘é¡µå¯¹æ¯”ï¼‰==========
    with open(news_list_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("åŒèŠ±é¡º 7x24 å°æ—¶è¦é—» - å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºä¸åŸç½‘é¡µå¯¹æ¯”ï¼‰\n")
        f.write("=" * 80 + "\n")
        f.write(f"å¯¼å‡ºæ—¶é—´: {crawler.get_cn_time()}\n")
        f.write(f"æ€»æ¡æ•°: {len(crawler.all_news)} æ¡\n")
        f.write("æ’åº: æŒ‰åºå·ä»å¤§åˆ°å°ï¼ˆæœ€æ–°åœ¨å‰ï¼‰\n")
        f.write("=" * 80 + "\n\n")

        # æŒ‰åºå·æ’åºï¼ˆä»å¤§åˆ°å° = æœ€æ–°åœ¨å‰ï¼‰
        sorted_news = sorted(crawler.all_news.values(), key=lambda x: x.get('seq', 0), reverse=True)

        for i, item in enumerate(sorted_news, 1):
            f.write(f"[{i:03d}] ----------------------------------------\n")
            f.write(f"åºå·: {item['seq']}\n")
            f.write(f"æ—¶é—´: {item['pub_date']}\n")
            f.write(f"æ ‡é¢˜: {item['title']}\n")
            f.write(f"é“¾æ¥: {item['url']}\n")
            if item['stock_code']:
                f.write(f"è‚¡ç¥¨: {item['stock_code']}\n")
            if item['implevel']:
                f.write(f"é‡è¦: {item['implevel']}\n")
            f.write(f"å†…å®¹: {item['content']}\n")
            f.write("\n")

        f.write("=" * 80 + "\n")
        f.write("åˆ—è¡¨ç»“æŸ\n")
        f.write("=" * 80 + "\n")

    print(f"\næ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    print(f"æ¶ˆæ¯åˆ—è¡¨å·²ä¿å­˜åˆ°: {news_list_file}")


if __name__ == "__main__":
    main()
