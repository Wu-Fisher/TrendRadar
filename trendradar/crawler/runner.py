# coding=utf-8
"""
çˆ¬è™«è¿è¡Œå™¨

å°†è‡ªå®šä¹‰çˆ¬è™«é›†æˆåˆ° TrendRadar ä¸»æµç¨‹ã€‚
æ”¯æŒï¼š
- 10ç§’è½®è¯¢
- å¢é‡æ£€æµ‹
- è¿‡æ»¤æ ‡è®°
- ç½‘é¡µå±•ç¤º
- æ¨é€é€šçŸ¥
"""

import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Callable, Any
import json

from trendradar.logging import get_logger
from .custom import (
    CrawlerManager,
    THSCrawler,
    THSTappCrawler,
    CrawlerNewsItem,
    CrawlResult,
    FetchStatus,
    filter_news_items,
    load_frequency_words_for_crawler,
    format_filter_result,
)

logger = get_logger(__name__)


class CrawlerRunner:
    """çˆ¬è™«è¿è¡Œå™¨

    æä¾›ä¸ TrendRadar ä¸»æµç¨‹çš„é›†æˆæ¥å£ã€‚
    """

    def __init__(self, config: Dict, ctx=None):
        """åˆå§‹åŒ–è¿è¡Œå™¨

        Args:
            config: é…ç½®å­—å…¸
            ctx: AppContext å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config
        self.ctx = ctx

        # çˆ¬è™«é…ç½®
        crawler_config = config.get("CRAWLER_CUSTOM", {})
        self.enabled = crawler_config.get("ENABLED", True)
        self.poll_interval = crawler_config.get("POLL_INTERVAL", 10)

        # å†…å®¹è·å–é…ç½®
        full_content_config = crawler_config.get("FULL_CONTENT", {})
        self.content_enabled = full_content_config.get("ENABLED", True)
        self.content_async = full_content_config.get("ASYNC_MODE", True)
        self.content_fetch_delay = full_content_config.get("FETCH_DELAY", 0.3)

        # å­˜å‚¨é…ç½®
        storage_config = crawler_config.get("STORAGE", {})
        self.max_items = storage_config.get("MAX_ITEMS", 10000)
        self.max_days = storage_config.get("MAX_DAYS", 30)
        self.max_display_items = storage_config.get("MAX_DISPLAY_ITEMS", 100)

        # æ•°æ®åº“è·¯å¾„
        data_dir = config.get("STORAGE", {}).get("LOCAL", {}).get("DATA_DIR", "output")
        db_dir = Path(data_dir) / "crawler"
        db_dir.mkdir(parents=True, exist_ok=True)
        self.db_path = str(db_dir / "crawler.db")

        # åˆ›å»ºç®¡ç†å™¨
        self.manager = CrawlerManager(
            config={
                "poll_interval": self.poll_interval,
                "full_content": {
                    "enabled": self.content_enabled,
                    "async_mode": self.content_async,
                    "fetch_delay": self.content_fetch_delay,
                },
            },
            db_path=self.db_path,
        )

        # æ³¨å†Œçˆ¬è™«
        self._register_crawlers(crawler_config.get("SOURCES", []))

        # è¿‡æ»¤é…ç½®
        self.filter_enabled = crawler_config.get("FILTER", {}).get("ENABLED", True)
        self.show_filter_tag = crawler_config.get("FILTER", {}).get("SHOW_TAG", True)

        # å›è°ƒ
        self._on_new_items: List[Callable] = []
        self._on_filtered: List[Callable] = []

        # çŠ¶æ€
        self._running = False
        self._poll_thread = None
        self._last_results: Dict[str, CrawlResult] = {}
        self._last_items: Dict[str, List[CrawlerNewsItem]] = {}  # ä¿å­˜è¿‡æ»¤åçš„æ¡ç›®ï¼ˆå«è¿‡æ»¤æ ‡è®°ï¼‰

    def _register_crawlers(self, sources: List[Dict]) -> None:
        """æ³¨å†Œé…ç½®çš„çˆ¬è™«"""
        # è·å–å…¨å±€ API ç±»å‹é…ç½®
        crawler_config = self.config.get("CRAWLER_CUSTOM", {})
        default_api_type = crawler_config.get("API_TYPE", "tapp")  # é»˜è®¤ä½¿ç”¨ TAPP API

        if not sources:
            # é»˜è®¤æ³¨å†ŒåŒèŠ±é¡ºçˆ¬è™«
            sources = [{"id": "ths-realtime", "name": "åŒèŠ±é¡º7x24", "type": "ths", "enabled": True}]

        for source in sources:
            if not source.get("enabled", True):
                continue

            source_type = source.get("type", "ths")
            # æ”¯æŒåœ¨ source çº§åˆ«è¦†ç›– api_type
            api_type = source.get("api_type", default_api_type)

            if source_type in ("ths", "ths_tapp"):
                crawler_cls = THSTappCrawler if api_type == "tapp" else THSCrawler
                crawler = crawler_cls(config={
                    "timezone": self.config.get("TIMEZONE", "Asia/Shanghai"),
                    "timeout": source.get("timeout", 10),
                    "content_fetch_delay": self.content_fetch_delay,
                    "page_size": source.get("page_size", 100),
                })
                self.manager.register(crawler)
                logger.info("æ³¨å†Œ %s (API: %s)", source.get('name', source_type), api_type)

    def crawl_once(self) -> Dict[str, CrawlResult]:
        """æ‰§è¡Œä¸€æ¬¡çˆ¬å–

        Returns:
            {source_id: CrawlResult} å­—å…¸
        """
        if not self.enabled:
            return {}

        results = self.manager.crawl_all()
        self._last_results = results

        # å¤„ç†æ¯ä¸ªç»“æœ
        for source_id, result in results.items():
            if result.status != FetchStatus.SUCCESS:
                logger.warning("%s è·å–å¤±è´¥: %s", source_id, result.error_message)
                continue

            logger.info("%s è·å–æˆåŠŸ: %d æ¡, æ–°å¢ %d æ¡", source_id, result.total_count, result.new_count)

            # è·å–å®Œæ•´å†…å®¹ï¼ˆæ–°å¢æ¡ç›®ï¼‰
            if self.content_enabled and result.new_count > 0:
                new_items = [item for item in result.items if item.seq in
                           (self.manager.seen_items.get(source_id, set()) -
                            set(item.seq for item in self._get_old_items(source_id, result.items)))]

                # å®é™…ä¸Šè·å–æ‰€æœ‰æœªè·å–å†…å®¹çš„æ¡ç›®
                items_to_fetch = [item for item in result.items if not item.content_fetched]
                if items_to_fetch:
                    logger.info("å¼€å§‹è·å– %d æ¡æ–°é—»çš„å®Œæ•´å†…å®¹...", len(items_to_fetch))
                    self.manager.fetch_full_content(
                        source_id,
                        items_to_fetch,
                        async_mode=self.content_async,
                    )

            # è¿‡æ»¤å¤„ç†
            if self.filter_enabled:
                self._apply_filter(source_id, result.items)
            else:
                # ä¸è¿‡æ»¤æ—¶ï¼Œæ‰€æœ‰æ¡ç›®æ ‡è®°ä¸ºé€šè¿‡
                for item in result.items:
                    item.filtered_out = False
                    item.matched_keywords = []

            # è¿‡æ»¤åé‡æ–°ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæ›´æ–°è¿‡æ»¤çŠ¶æ€ï¼‰
            self.manager._save_items(source_id, result.source_name, result.items)

            # ä¿å­˜æ¡ç›®åˆ°å†…å­˜ï¼ˆå«è¿‡æ»¤æ ‡è®°ï¼‰
            self._last_items[source_id] = result.items

        return results

    def _get_old_items(self, source_id: str, current_items: List[CrawlerNewsItem]) -> List[CrawlerNewsItem]:
        """è·å–æ—§æ¡ç›®ï¼ˆç”¨äºè®¡ç®—æ–°å¢ï¼‰"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…æ–°å¢æ£€æµ‹åœ¨ manager ä¸­å®Œæˆ
        return []

    def _apply_filter(self, source_id: str, items: List[CrawlerNewsItem]) -> None:
        """åº”ç”¨è¿‡æ»¤"""
        try:
            # åŠ è½½å…³é”®è¯é…ç½®
            word_groups, filter_words, global_filters = load_frequency_words_for_crawler()

            if not word_groups and not filter_words and not global_filters:
                # æ— è¿‡æ»¤é…ç½®ï¼Œå…¨éƒ¨é€šè¿‡
                for item in items:
                    item.filtered_out = False
                    item.matched_keywords = []
                return

            # æ‰§è¡Œè¿‡æ»¤
            passed, filtered = filter_news_items(
                items, word_groups, filter_words, global_filters
            )

            logger.info("è¿‡æ»¤ç»“æœ: é€šè¿‡ %d æ¡, è¿‡æ»¤ %d æ¡", len(passed), len(filtered))

            # è§¦å‘å›è°ƒ
            for callback in self._on_filtered:
                try:
                    callback(source_id, passed, filtered)
                except Exception as e:
                    logger.error("è¿‡æ»¤å›è°ƒé”™è¯¯: %s", e)

        except Exception as e:
            logger.error("è¿‡æ»¤å¤„ç†é”™è¯¯: %s", e)

    def start_polling(self, callback: Optional[Callable] = None) -> None:
        """å¼€å§‹è½®è¯¢

        Args:
            callback: æ¯æ¬¡çˆ¬å–åçš„å›è°ƒå‡½æ•°
        """
        if self._running:
            return

        self._running = True

        def poll_task():
            while self._running:
                try:
                    results = self.crawl_once()
                    if callback:
                        callback(results)
                except Exception as e:
                    logger.error("è½®è¯¢é”™è¯¯: %s", e)

                # ç­‰å¾…ä¸‹ä¸€æ¬¡
                for _ in range(self.poll_interval):
                    if not self._running:
                        break
                    time.sleep(1)

        self._poll_thread = threading.Thread(target=poll_task, daemon=True)
        self._poll_thread.start()
        logger.info("å¼€å§‹è½®è¯¢ï¼Œé—´éš” %d ç§’", self.poll_interval)

    def stop_polling(self) -> None:
        """åœæ­¢è½®è¯¢"""
        self._running = False
        if self._poll_thread:
            self._poll_thread.join(timeout=5)
        logger.info("åœæ­¢è½®è¯¢")

    def get_items_for_display(
        self,
        source_id: Optional[str] = None,
        include_filtered: bool = True,
        max_items: int = None
    ) -> List[Dict[str, Any]]:
        """è·å–ç”¨äºå±•ç¤ºçš„æ¡ç›®åˆ—è¡¨

        Args:
            source_id: æ•°æ®æº IDï¼ˆå¯é€‰ï¼‰
            include_filtered: æ˜¯å¦åŒ…å«è¢«è¿‡æ»¤çš„æ¡ç›®
            max_items: æœ€å¤§æ¡ç›®æ•°

        Returns:
            æ¡ç›®åˆ—è¡¨ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        """
        max_items = max_items or self.max_display_items

        # ä¼˜å…ˆä½¿ç”¨å†…å­˜ä¸­çš„æ¡ç›®ï¼ˆå«è¿‡æ»¤æ ‡è®°ï¼‰
        items = []
        if self._last_items:
            if source_id:
                items = self._last_items.get(source_id, [])
            else:
                for src_items in self._last_items.values():
                    items.extend(src_items)
        else:
            # å›é€€åˆ°æ•°æ®åº“
            items = self.manager.get_items(
                source_id=source_id,
                limit=max_items,
                filtered_only=not include_filtered,
            )

        # è¿‡æ»¤å¤„ç†
        if not include_filtered:
            items = [item for item in items if not item.filtered_out]

        # æˆªå–æ˜¾ç¤ºæ•°é‡
        items = items[:max_items]

        result = []
        new_seqs = self._get_new_seqs()
        for item in items:
            item_dict = item.to_dict()
            # æ·»åŠ å±•ç¤ºç›¸å…³å­—æ®µ
            item_dict["is_new"] = item.seq in new_seqs
            item_dict["filter_tag"] = self._get_filter_tag(item) if self.show_filter_tag else ""
            result.append(item_dict)

        return result

    def _get_new_seqs(self) -> set:
        """è·å–æœ¬æ¬¡æ–°å¢çš„åºå·é›†åˆ"""
        seqs = set()
        for result in self._last_results.values():
            if result.status == FetchStatus.SUCCESS:
                # æœ€æ–°çš„æ¡ç›®è§†ä¸ºæ–°å¢
                for item in result.items[:result.new_count]:
                    seqs.add(item.seq)
        return seqs

    def _get_filter_tag(self, item: CrawlerNewsItem) -> str:
        """è·å–è¿‡æ»¤æ ‡ç­¾"""
        if item.filtered_out:
            return f"ğŸš« {item.filter_reason}"
        elif item.matched_keywords:
            return f"âœ“ {', '.join(item.matched_keywords)}"
        return ""

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.manager.get_stats()
        return {
            source_id: {
                "source_id": s.source_id,
                "total_fetches": s.total_fetches,
                "successful_fetches": s.successful_fetches,
                "failed_fetches": s.failed_fetches,
                "total_items": s.total_items,
                "new_items": s.new_items,
                "last_fetch_time": s.last_fetch_time,
                "last_success_time": s.last_success_time,
                "last_error": s.last_error,
            }
            for source_id, s in stats.items()
        }

    def get_errors(self, limit: int = 50) -> List[Dict]:
        """è·å–é”™è¯¯æ—¥å¿—"""
        errors = self.manager.get_errors(limit=limit)
        return [e.to_dict() for e in errors]

    def on_new_items(self, callback: Callable) -> None:
        """æ³¨å†Œæ–°æ¡ç›®å›è°ƒ"""
        self._on_new_items.append(callback)
        self.manager.on_new_items(lambda sid, items: callback(sid, items))

    def on_filtered(self, callback: Callable) -> None:
        """æ³¨å†Œè¿‡æ»¤å®Œæˆå›è°ƒ"""
        self._on_filtered.append(callback)

    def convert_to_rss_format(
        self,
        items: List[CrawlerNewsItem]
    ) -> List[Dict[str, Any]]:
        """å°†çˆ¬è™«æ¡ç›®è½¬æ¢ä¸º RSS æ ¼å¼ï¼ˆå…¼å®¹ç°æœ‰æ¨é€é€»è¾‘ï¼‰

        Args:
            items: çˆ¬è™«æ¡ç›®åˆ—è¡¨

        Returns:
            RSS æ ¼å¼çš„æ¡ç›®åˆ—è¡¨
        """
        rss_items = []
        for item in items:
            rss_item = {
                "title": item.title,
                "feed_id": "ths-realtime",
                "feed_name": "åŒèŠ±é¡º7x24",
                "url": item.url,
                "published_at": item.published_at,
                "summary": item.summary or item.full_content[:200] if item.full_content else "",
                "author": item.source,
                "full_content": item.full_content,
                # è¿‡æ»¤ç›¸å…³
                "matched_keywords": item.matched_keywords,
                "filtered_out": item.filtered_out,
                "filter_reason": item.filter_reason,
                # æ‰©å±•ä¿¡æ¯
                "extra": item.extra,
            }
            rss_items.append(rss_item)
        return rss_items

    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.stop_polling()
        # æ¸…ç†æ—§æ•°æ®
        deleted = self.manager.cleanup_old_data(
            max_items=self.max_items,
            max_days=self.max_days,
        )
        if deleted > 0:
            logger.info("æ¸…ç†äº† %d æ¡æ—§æ•°æ®", deleted)
        self.manager.cleanup()


def create_crawler_runner(config: Dict, ctx=None) -> CrawlerRunner:
    """åˆ›å»ºçˆ¬è™«è¿è¡Œå™¨

    Args:
        config: é…ç½®å­—å…¸
        ctx: AppContext å®ä¾‹

    Returns:
        CrawlerRunner å®ä¾‹
    """
    return CrawlerRunner(config, ctx)
